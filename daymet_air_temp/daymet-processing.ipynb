{"cells":[{"cell_type":"markdown","metadata":{"id":"2cHfhBuXZAHY"},"source":["# Setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5wOn_DfZeVy"},"outputs":[],"source":["import geopandas as gpd\n","import pandas as pd\n","import numpy as np\n","import csv as csv\n","import requests\n","from requests.exceptions import RequestException\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from io import StringIO\n","import os\n","import time\n"]},{"cell_type":"markdown","metadata":{"id":"w15QOuh8ZCz6"},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldtC2hT-4ddX"},"outputs":[],"source":["# import prison boundaries as shapefile from Department of Homeland Security\n","prisonsRaw = gpd.read_file('https://opendata.arcgis.com/api/v3/datasets/2d6109d4127d458eaf0958e4c5296b67_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1')\n","\n","# Load the clean list of prisons from a CSV file, ensuring 'FACILITYID' is read as a string\n","prisonsClean = pd.read_csv('https://raw.githubusercontent.com/callmeufu/prison_environmental_justice/main/prison_datasets/state_fed_prisons.csv', dtype={'FACILITYID': str})\n","# prisonsClean = pd.read_csv('../prison_datasets/state_fed_prisons.csv', dtype={'FACILITYID': str})\n","\n","# Ensure 'FACILITYID' in the raw prisons data is treated as a string\n","prisonsRaw['FACILITYID'] = prisonsRaw['FACILITYID'].astype(str)\n","\n","# Filter the raw prisons data to include only those records with 'FACILITYID' present in the clean list\n","filtered_prisons = prisonsRaw[prisonsRaw['FACILITYID'].isin(prisonsClean['FACILITYID'])]\n","\n","# Create a deep copy of the filtered prisons data for further processing\n","prisonsFinal = filtered_prisons.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlw1q6fvPsNJ"},"outputs":[],"source":["# Reproject the dataset to the EPSG:2163 coordinate reference system\n","# Calculating centroids in a projected coordinate system ensures that \n","# the centroids are calculated in a planar coordinate system, which is \n","# more accurate than calculating them in a geographic coordinate system.\n","\n","prisons_projected = prisonsFinal.to_crs(epsg=2163)\n","\n","# Calculate the centroids of the reprojected geometries\n","centroids_projected = prisons_projected.geometry.centroid\n","\n","# Create a new GeoDataFrame containing the original attributes and the computed centroids\n","prisons_centroids = gpd.GeoDataFrame(prisonsFinal[['FID', 'FACILITYID', 'NAME', 'ADDRESS', 'CITY', 'STATE', 'ZIP', 'ZIP4',\n","       'TELEPHONE', 'TYPE', 'STATUS', 'POPULATION', 'COUNTY', 'COUNTYFIPS',\n","       'COUNTRY', 'NAICS_CODE', 'NAICS_DESC', 'SOURCE', 'SOURCEDATE',\n","       'VAL_METHOD', 'VAL_DATE', 'WEBSITE', 'SECURELVL', 'CAPACITY',\n","       'SHAPE_Leng', 'GlobalID', 'CreationDate', 'Creator', 'EditDate',\n","       'Editor', 'SHAPE_Length', 'SHAPE_Area', 'geometry']], geometry=centroids_projected)\n","\n","# Reproject the centroids GeoDataFrame back to WGS84 (EPSG 4326), the default CRS of daymet for coordinate queries \n","prisons_centroids = prisons_centroids.to_crs(epsg=4326)"]},{"cell_type":"markdown","metadata":{"id":"GjBnObo7ZELu"},"source":["## Daymet Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4699414,"status":"ok","timestamp":1714500387473,"user":{"displayName":"Ufuoma Ovienmhada","userId":"14257389769286656983"},"user_tz":240},"id":"D6zeeCNmAz3T","outputId":"2a68690c-c214-4800-b37d-850bb8131b80"},"outputs":[],"source":["# --- GLOBAL VARIABLES\n","# variables to be used to compute date range\n","first_day_summer = '-06-01'\n","last_day_summer = '-08-31'\n","begin_year = 1990\n","end_year = 2023\n","\n","# variables that are used between functions\n","prisons_shapes = None\n","time_series = None\n","csv_writer = None\n","\n","# --- FUNCTIONS\n","\n","# Function to convert Celsius to Fahrenheit\n","# Input: single numerical value in Celsius\n","# Output: single numerical value in Fahrenheit\n","def celsius_to_fahrenheit(celsius):\n","   fahrenheit = (1.8 * celsius) + 32\n","   return fahrenheit\n","\n","# Function to calculate the mean temperature of the day in Fahrenheit\n","# Input: day_tmin -> list of minimum daily temperatures in Celsius\n","#        day_tmax -> list of maximum daily temperatures in Celsius\n","# Output: mean daily temperature in Fahrenheit\n","def calc_mean(day_tmin, day_tmax):\n","    # Convert tmin and tmax to Fahrenheit and calculate the mean\n","    current_tmean = (celsius_to_fahrenheit(min(day_tmin[0])) + celsius_to_fahrenheit(min(day_tmax[0]))) / 2\n","    return current_tmean\n","\n","# Function to estimate the number of extreme heat days\n","# Input: daily_stat -> single numerical value, daily temperature\n","#        threshold_temp -> single numerical value, threshold temperature\n","# Output: 1 if daily_stat is above threshold_temp, otherwise 0\n","def estimate_extreme_heat(daily_stat, threshold_temp):\n","    # Check if daily_stat is above threshold_temp\n","    above_threshold = 1 if daily_stat > threshold_temp else 0\n","    return above_threshold\n","\n","# Function to calculate the number of heatwaves based on daily maximum temperatures\n","# heatwave is defined as a period where the daily maximum temperature exceeds the \n","# 90th percentile of maximum temperatures for the given period. \n","# Input: data -> DataFrame with daily temperature data, must include 'tmax' column\n","#        percentile_90th -> 90th percentile value of daily maximum temperatures\n","# Output: heatwave1 -> count of single-day heatwaves\n","#         heatwave2 -> count of two-day heatwaves\n","#         heatwave3 -> count of three-or-more-day heatwaves\n","def calculate_heatwaves(data, percentile_90th):\n","    # Determine which days have maximum temperatures above the 90th percentile\n","    above_90 = data['tmax'] > percentile_90th\n","\n","    # Initialize counts for different heatwave lengths\n","    heatwave1, heatwave2, heatwave3 = 0, 0, 0\n","    current_streak = 0  # Counter for current streak of consecutive days above 90th percentile\n","\n","    # Iterate over each day in the group\n","    for i in range(len(data)):\n","        if above_90.iloc[i]:  # If the temperature is above the 90th percentile\n","            current_streak += 1  # Increment the streak counter\n","        else:  # If the temperature is not above the 90th percentile\n","            # Count the completed heatwave based on its length\n","            if current_streak == 1:\n","                heatwave1 += 1\n","            elif current_streak == 2:\n","                heatwave2 += 1\n","            elif current_streak >= 3:\n","                heatwave3 += 1\n","            current_streak = 0  # Reset the streak counter\n","\n","    # Handle the case where the last day(s) form a heatwave\n","    if current_streak == 1:\n","        heatwave1 += 1\n","    elif current_streak == 2:\n","        heatwave2 += 1\n","    elif current_streak >= 3:\n","        heatwave3 += 1\n","\n","    return heatwave1, heatwave2, heatwave3\n","\n","# Function to retrieve and process heat statistics for a given prison over a range of years\n","# Input: prison_index -> index of the prison in the GeoDataFrame\n","#        prisons_shapes -> GeoDataFrame containing prison geometries, in this case lat/lon\n","#        begin_year -> start year for the data retrieval\n","#        end_year -> end year for the data retrieval\n","#        max_retries -> maximum number of retries for HTTP requests\n","#        retry_delay -> delay between retries in seconds\n","# Output: DataFrame containing processed heat statistics for the specified range of years\n","def get_heat_stats(prison_index, prisons_shapes, begin_year, end_year, max_retries=15, retry_delay=20):\n","    all_year_results = []\n","\n","    # Get the geometry (coordinates) of the specified prison\n","    coords = prisons_shapes.iloc[prison_index].geometry\n","    lat, lon = coords.y, coords.x\n","\n","    # Loop over each year in the specified range\n","    for year in range(begin_year, end_year + 1):\n","        retries = 0  # Initialize retry counter\n","        while retries <= max_retries:\n","            \n","            # Define the start and end dates to iterate across\n","            start_date = f\"{year}{first_day_summer}\"\n","            end_date = f\"{year}{last_day_summer}\"\n","            \n","            # Construct the URL for the API request\n","            # For each prison and date, grab tmax, tmin, and vp\n","            url = f\"https://daymet.ornl.gov/single-pixel/api/data?lat={lat}&lon={lon}&vars=tmax,tmin,vp&start={start_date}&end={end_date}\"\n","            \n","            try:\n","                # Make the API request\n","                response = requests.get(url, timeout=20)\n","\n","                if response.ok:\n","                    # Split the response text into lines\n","                    lines = response.text.split('\\n')\n","\n","                    # Find the index of the line where the CSV data starts (where the header 'year' is found)\n","                    start_line = next((i for i, line in enumerate(lines) if line.startswith('year')), None)\n","\n","                    # Ensure that the start_line is valid and there are lines following the header\n","                    if start_line is not None and start_line < len(lines) - 1:\n","                        \n","                        # Extract the CSV data from the response starting from the header line\n","                        csv_data = '\\n'.join(lines[start_line:])\n","\n","                        # Read the CSV data into a DataFrame\n","                        data = pd.read_csv(StringIO(csv_data))\n","\n","                        # Convert temperature values from Celsius to Fahrenheit\n","                        data['tmax'] = data['tmax (deg c)'].apply(celsius_to_fahrenheit)\n","                        data['tmin'] = data['tmin (deg c)'].apply(celsius_to_fahrenheit)\n","\n","                        # Extract daily maximum temperatures and calculate daily mean temperatures\n","                        daily_maxs = data['tmax'].tolist()\n","                        daily_means = data[['tmax', 'tmin']].mean(axis=1).tolist()\n","                        \n","                        # Calculate summer statistics\n","                        summer_temp_mean = np.mean(daily_means)\n","                        summer_tmax_mean = np.mean(daily_maxs)\n","                        summer_90th_percent = np.percentile(daily_maxs, 90)\n","                        count_days_above_10F = sum(1 for mean in daily_means if estimate_extreme_heat(mean - summer_temp_mean, 10))\n","                        count_days_above_85F = sum(1 for mean in daily_means if estimate_extreme_heat(mean, 85))\n","\n","                        # Calculate heatwaves using the precomputed 90th percentile\n","                        heatwave1, heatwave2, heatwave3 = calculate_heatwaves(data, summer_90th_percent)\n","\n","                        all_year_results.append({\n","                            'FACILITYID': prisons_shapes.iloc[prison_index]['FACILITYID'],\n","                            'NAME': prisons_shapes.iloc[prison_index]['NAME'],\n","                            'STATE': prisons_shapes.iloc[prison_index]['STATE'],\n","                            'YEAR': year,\n","                            'SUMMER_TEMP_MEAN': summer_temp_mean,\n","                            'SUMMER_TMAX_MEAN': summer_tmax_mean,\n","                            'SUMMER_90TH_PERCENT': summer_90th_percent,\n","                            'COUNT_DAYS_ABOVE_10F': count_days_above_10F,\n","                            'COUNT_DAYS_ABOVE_85F': count_days_above_85F,\n","                            'HEATWAVE1': heatwave1,\n","                            'HEATWAVE2': heatwave2,\n","                            'HEATWAVE3': heatwave3,\n","                        })\n","                    else:\n","                        print(f\"Unexpected data format for year {year} at index {prison_index}.\")\n","                    break  # Successful request, break the retry loop\n","                else:\n","                    print(f\"Request failed for {start_date} to {end_date} at index {prison_index} with status code {response.status_code}. URL: {url}\")\n","                    break  # Non-retryable error (e.g., bad request), break the retry loop\n","\n","            except RequestException as e:\n","                print(f\"Timeout or RequestException for {year} at index {prison_index}: {e}.\")\n","                retries += 1\n","                if retries > max_retries:\n","                    print(\"Max retries exceeded, moving to next year.\")\n","                    break  # Max retries exceeded, move to the next year\n","                else:\n","                    print(\"Retrying...\")\n","                    time.sleep(retry_delay)  # Wait before retrying\n","\n","            except pd.errors.ParserError:\n","                print(f\"ParserError: Error parsing CSV data for year {year} at index {prison_index}. URL: {url}\")\n","                break  # No retry for parsing errors\n","\n","            except Exception as e:\n","                print(f\"Unexpected error processing data for year {year} at index {prison_index}: {e}. URL: {url}\")\n","                break  # No retry for unexpected errors\n","\n","    return pd.DataFrame(all_year_results)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to split a DataFrame into smaller chunks\n","# Input: df -> DataFrame to be split\n","#        chunk_size -> number of rows per chunk\n","# Output: List of DataFrame chunks\n","def split_dataframe(df, chunk_size):\n","    chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n","    return chunks\n","\n","# Function to process a dataset chunk and retrieve heat statistics\n","# Input: dataset -> DataFrame chunk to be processed\n","#        start_year -> start year for the data retrieval\n","#        end_year -> end year for the data retrieval\n","#        area_name -> name of the area being processed (for logging)\n","# Output: DataFrame with processed heat statistics for the chunk\n","def process_dataset(dataset, start_year, end_year, area_name):\n","    print(f\"Processing chunk with {len(dataset)} records.\")  # Log processing start of a chunk\n","    results = []\n","    with ThreadPoolExecutor(max_workers=4) as executor: # Use ThreadPoolExecutor to parallelize processing\n","        # Submit get_heat_stats tasks to the executor for each index in the dataset\n","        future_to_index = {executor.submit(get_heat_stats, idx, dataset, start_year, end_year): idx for idx in range(len(dataset))}\n","        for future in as_completed(future_to_index):\n","            result = future.result()\n","            if result is not None:\n","                results.append(result)\n","            else:\n","                # Handle None result if necessary\n","                pass\n","    # Concatenate results into a single DataFrame\n","    df = pd.concat(results, ignore_index=True)\n","    return df\n","\n","# Split the prisons_centroids DataFrame into 10 chunks\n","prisons_centroids_chunks = split_dataframe(prisons_centroids, chunk_size=np.ceil(len(prisons_centroids) / 15).astype(int))\n","\n","processed_chunks = []\n","for chunk_idx, chunk in enumerate(prisons_centroids_chunks):\n","    # Log processing of each chunk\n","    print(f\"Processing chunk {chunk_idx+1}/{len(prisons_centroids_chunks)}\")\n","    processed_chunk = process_dataset(chunk, begin_year, end_year, area_name=\"Complete_Dataset\")\n","    processed_chunks.append(processed_chunk)\n","\n","# Combine all processed chunks into a single DataFrame\n","combined_data = pd.concat(processed_chunks, ignore_index=True)\n","# Save the combined DataFrame to a CSV file\n","combined_data.to_csv('./Output/combined_processed_prisons.csv')\n","# combined_data.to_csv(os.path.join(PATH_MAIN,'Output/processed_prisons_1990_2023.csv'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define states for the study\n","study_area = ['AZ', 'WA', 'MA', 'CA', 'FL']\n","\n","# Filter the DataFrame to include only the rows where 'STATE' is in the study area list\n","filtered_data = combined_data[combined_data['STATE'].isin(study_area)]\n","\n","# Save the filtered study area data to a new CSV\n","filtered_data.to_csv('./Output/study_area.csv', index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
