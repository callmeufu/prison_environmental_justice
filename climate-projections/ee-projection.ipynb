{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='ee-mrk2152')\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('centroids.csv')\n",
    "\n",
    "# Convert the 'geometry' column to actual geometric points\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Extract latitude and longitude from the geometry column\n",
    "gdf['latitude'] = gdf.geometry.y\n",
    "gdf['longitude'] = gdf.geometry.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROJECTION DATA\n",
    "\n",
    "# Create reduce region function\n",
    "def create_reduce_region_function(geometry, reducer=ee.Reducer.mean(), scale=1000, crs='EPSG:4326', bestEffort=True, maxPixels=1e13, tileScale=4):\n",
    "    def reduce_region_function(img):\n",
    "        stat = img.reduceRegion(\n",
    "            reducer=reducer,\n",
    "            geometry=geometry,\n",
    "            scale=scale,\n",
    "            crs=crs,\n",
    "            bestEffort=bestEffort,\n",
    "            maxPixels=maxPixels,\n",
    "            tileScale=tileScale\n",
    "        )\n",
    "        return ee.Feature(geometry, stat).set({'millis': img.date().millis()})\n",
    "    return reduce_region_function\n",
    "\n",
    "# Convert feature collection to dictionary\n",
    "def fc_to_dict(fc):\n",
    "    prop_names = fc.first().propertyNames()\n",
    "    prop_lists = fc.reduceColumns(\n",
    "        reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
    "        selectors=prop_names\n",
    "    ).get('list')\n",
    "    return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
    "\n",
    "# Add date information to DataFrame\n",
    "def add_date_info(df):\n",
    "    df['Timestamp'] = pd.to_datetime(df['millis'], unit='ms')\n",
    "    df['Year'] = pd.DatetimeIndex(df['Timestamp']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['Timestamp']).month\n",
    "    df['Day'] = pd.DatetimeIndex(df['Timestamp']).day\n",
    "    df['DOY'] = pd.DatetimeIndex(df['Timestamp']).dayofyear\n",
    "    return df\n",
    "\n",
    "# Filter summer months (june/july/august)\n",
    "def filter_summer_months(df):\n",
    "    return df[df['Month'].isin([6, 7, 8])]\n",
    "\n",
    "# Define the ImageCollection with Earth Engine. Details on RCPs in markdown above. For this sample, I set it to intermediate scenario rcp45 which most\n",
    "# sources have called the most likely of the set... It declares peak emissions happen at 20240...should double check and get official source on which scenario sleected\n",
    "\n",
    "dcp_col = (ee.ImageCollection('NASA/NEX-DCP30_ENSEMBLE_STATS')\n",
    "           .select(['tasmax_median', 'tasmin_median'])\n",
    "           .filter(\n",
    "               ee.Filter.And(ee.Filter.eq('scenario', 'rcp45'),\n",
    "                             ee.Filter.date('2024-07-01', '2074-12-31'))))\n",
    "\n",
    "# Calculate mean temperature\n",
    "def calc_mean_temp(img):\n",
    "    return (img.select('tasmax_median')\n",
    "            .add(img.select('tasmin_median'))\n",
    "            .divide(ee.Image.constant(2.0))\n",
    "            .rename(['Temp-mean'])\n",
    "            .copyProperties(img, img.propertyNames()))\n",
    "\n",
    "dcp_col = dcp_col.map(calc_mean_temp)\n",
    "\n",
    "# Process a single facility\n",
    "def process_facility(row):\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    point = ee.Geometry.Point([longitude, latitude])\n",
    "    facility_id = row['FACILITYID']\n",
    "    name = row['NAME']\n",
    "\n",
    "    # Reduce region function for this point\n",
    "    reduce_dcp30 = create_reduce_region_function(\n",
    "        geometry=point, reducer=ee.Reducer.first(), scale=1000, crs='EPSG:4326')\n",
    "\n",
    "    # Query ee data\n",
    "    dcp_stat_fc = ee.FeatureCollection(dcp_col.map(reduce_dcp30)).filter(\n",
    "        ee.Filter.notNull(dcp_col.first().bandNames()))\n",
    "\n",
    "    dcp_dict = fc_to_dict(dcp_stat_fc).getInfo()\n",
    "    dcp_df = pd.DataFrame(dcp_dict)\n",
    "\n",
    "    # Process ee data\n",
    "    dcp_df = add_date_info(dcp_df)\n",
    "    dcp_df['Temp-mean'] = dcp_df['Temp-mean'] - 273.15\n",
    "    dcp_df['Model'] = 'NEX-DCP30'\n",
    "    dcp_df['Temp-mean'] = (dcp_df['Temp-mean'] * (9/5)) + 32\n",
    "    dcp_df = filter_summer_months(dcp_df)\n",
    "    dcp_df = dcp_df.drop(['DOY', 'Day', 'Month', 'millis'], axis=1)\n",
    "    summer_mean_df = dcp_df.groupby(['Year']).mean(['Temp-mean']).reset_index()\n",
    "\n",
    "    # Add columns for id\n",
    "    summer_mean_df['facility_id'] = facility_id\n",
    "    summer_mean_df['name'] = name\n",
    "    summer_mean_df['latitude'] = latitude\n",
    "    summer_mean_df['longitude'] = longitude\n",
    "\n",
    "    return summer_mean_df\n",
    "\n",
    "# Read in centroid dataset\n",
    "df = pd.read_csv('centroids.csv')\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Extract lat lon\n",
    "gdf['latitude'] = gdf.geometry.y\n",
    "gdf['longitude'] = gdf.geometry.x\n",
    "\n",
    "# Lit for results\n",
    "all_results = []\n",
    "\n",
    "# Retrieve total number of facilities for print statement\n",
    "total_facilities = len(gdf)\n",
    "\n",
    "# Process facilities (with timeout, retries, and logging)\n",
    "def process_with_timeout(idx, row, retries=3, timeout=30):\n",
    "    for attempt in range(retries):\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future = executor.submit(process_facility, row)\n",
    "            try:\n",
    "                summer_mean_df = future.result(timeout=timeout)\n",
    "                all_results.append(summer_mean_df)\n",
    "                print(f'Successfully retrieved data for Facility ID {row[\"FACILITYID\"]} (Facility {idx + 1} out of {total_facilities}).')\n",
    "                break\n",
    "            except TimeoutError:\n",
    "                print(f'Timeout retrieving data for Facility ID {row[\"FACILITYID\"]} (Facility {idx + 1} out of {total_facilities}), retrying... ({attempt + 1}/{retries})')\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)  \n",
    "            except Exception as e:\n",
    "                print(f'Error retrieving data for Facility ID {row[\"FACILITYID\"]} (Facility {idx + 1} out of {total_facilities}): {e}')\n",
    "                break\n",
    "\n",
    "# run process function\n",
    "for idx, row in gdf.iterrows():\n",
    "    process_with_timeout(idx, row)\n",
    "\n",
    "# Concatenate results \n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "final_df.to_csv('nexdcp30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
