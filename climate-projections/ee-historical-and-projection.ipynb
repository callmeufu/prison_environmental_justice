{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='ee-mrk2152')\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipoint Pull - Prison Dataset\n",
    "\n",
    "For context...\n",
    "\n",
    "Representative Concentration Pathways\n",
    "\n",
    "The Coupled Model Intercomparison Project Phase 5 (CMIP5) projections make use of Representative Concentration Pathways (RCPs), which are designed to provide plausible future scenarios of anthropogenic forcing spanning a range from a low emission scenario characterized by active mitigation (RCP 2.6), through two intermediate scenarios (RCP 4.5 and RCP6.0), to a high emission scenario (RCP 8.5).\n",
    "\n",
    "Each RCP is associated with plausible combinations of projected population growth, economic activity, energy intensity, and socio-economic development. The RCP scenarios were named based on their total radiative forcing by (or post) 2100.\n",
    "Summary:\n",
    "\n",
    "RCP2.6 represents a peak in radiative forcing at approximately 3 W/m2 mid-century before declining to 2.6 W/m2 by 2100.\n",
    "\n",
    "RCP4.5 represents a stabilization (without overshoot) in radiative forcing at 4.5 W/m2 post 2100.\n",
    "\n",
    "RCP6.0 represents a stabilization (without overshoot) in radiative forcing at 6 W/m2 post 2100.\n",
    "\n",
    "RCP8.5 represents a rise in radiative forcing to 8.5 W/m2 in 2100.\n",
    "\n",
    "These RCP scenarios serve as input to the Earth System Models, which simulate the climate system response and resulting climate conditions.\n",
    "\n",
    "Earth Engine parameters here - https://developers.google.com/earth-engine/datasets/catalog/NASA_NEX-DCP30\n",
    "\n",
    "Prism dataset spans different sources, given that it dates back to 1895.\n",
    "\n",
    "Prism datasheet - https://prism.oregonstate.edu/documents/PRISM_datasets.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('centroids.csv')\n",
    "\n",
    "# Convert the 'geometry' column to actual geometric points\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Extract latitude and longitude from the geometry column\n",
    "gdf['latitude'] = gdf.geometry.y\n",
    "gdf['longitude'] = gdf.geometry.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PROJECTION AND HISTORICAL DATA\n",
    "\n",
    "# Create reduce region function\n",
    "def create_reduce_region_function(geometry, reducer=ee.Reducer.mean(), scale=1000, crs='EPSG:4326', bestEffort=True, maxPixels=1e13, tileScale=4):\n",
    "    def reduce_region_function(img):\n",
    "        stat = img.reduceRegion(\n",
    "            reducer=reducer,\n",
    "            geometry=geometry,\n",
    "            scale=scale,\n",
    "            crs=crs,\n",
    "            bestEffort=bestEffort,\n",
    "            maxPixels=maxPixels,\n",
    "            tileScale=tileScale\n",
    "        )\n",
    "        return ee.Feature(geometry, stat).set({'millis': img.date().millis()})\n",
    "    return reduce_region_function\n",
    "\n",
    "# Convert feature collection to dictionary\n",
    "def fc_to_dict(fc):\n",
    "    prop_names = fc.first().propertyNames()\n",
    "    prop_lists = fc.reduceColumns(\n",
    "        reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
    "        selectors=prop_names\n",
    "    ).get('list')\n",
    "    return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
    "\n",
    "# Add date information to DataFrame\n",
    "def add_date_info(df):\n",
    "    df['Timestamp'] = pd.to_datetime(df['millis'], unit='ms')\n",
    "    df['Year'] = pd.DatetimeIndex(df['Timestamp']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['Timestamp']).month\n",
    "    df['Day'] = pd.DatetimeIndex(df['Timestamp']).day\n",
    "    df['DOY'] = pd.DatetimeIndex(df['Timestamp']).dayofyear\n",
    "    return df\n",
    "\n",
    "# Filter summer months (June/July/August)\n",
    "def filter_summer_months(df):\n",
    "    return df[df['Month'].isin([6, 7, 8])]\n",
    "\n",
    "# Calculate mean temperature\n",
    "def calc_mean_temp(img):\n",
    "    return (img.select('tasmax_median')\n",
    "            .add(img.select('tasmin_median'))\n",
    "            .divide(ee.Image.constant(2.0))\n",
    "            .rename(['Temp-mean'])\n",
    "            .copyProperties(img, img.propertyNames()))\n",
    "\n",
    "# Define the ImageCollection with Earth Engine. Details on RCPs in markdown above. For this sample, I set it to intermediate scenario rcp45 which most\n",
    "# sources have called the most likely of the set... It declares peak emissions happen at 20240...should double check and get official source on which scenario sleected\n",
    "dcp_col = (ee.ImageCollection('NASA/NEX-DCP30_ENSEMBLE_STATS')\n",
    "           .select(['tasmax_median', 'tasmin_median'])\n",
    "           .filter(\n",
    "               ee.Filter.And(ee.Filter.eq('scenario', 'rcp45'),\n",
    "                             ee.Filter.date('2024-07-01', '2074-12-31'))))\n",
    "\n",
    "dcp_col = dcp_col.map(calc_mean_temp)\n",
    "\n",
    "# PRISM data collection\n",
    "prism_col = (ee.ImageCollection('OREGONSTATE/PRISM/AN81m')\n",
    "             .select(['tmean'])\n",
    "             .filter(ee.Filter.date('1973-01-01', '2023-12-31')))\n",
    "\n",
    "def process_facility(row):\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    point = ee.Geometry.Point([longitude, latitude])\n",
    "    facility_id = row['FACILITYID']\n",
    "    name = row['NAME']\n",
    "\n",
    "    # Reduce region function for projected data\n",
    "    reduce_dcp30 = create_reduce_region_function(\n",
    "        geometry=point, reducer=ee.Reducer.first(), scale=1000, crs='EPSG:4326')\n",
    "\n",
    "    # Query projected data\n",
    "    dcp_stat_fc = ee.FeatureCollection(dcp_col.map(reduce_dcp30)).filter(\n",
    "        ee.Filter.notNull(dcp_col.first().bandNames()))\n",
    "\n",
    "    dcp_dict = fc_to_dict(dcp_stat_fc).getInfo()\n",
    "    dcp_df = pd.DataFrame(dcp_dict)\n",
    "\n",
    "    # Process projected data\n",
    "    dcp_df = add_date_info(dcp_df)\n",
    "    dcp_df['Temp-mean'] = dcp_df['Temp-mean'] - 273.15\n",
    "    dcp_df['Model'] = 'NEX-DCP30'\n",
    "    dcp_df['Temp-mean'] = (dcp_df['Temp-mean'] * (9/5)) + 32\n",
    "    dcp_df = filter_summer_months(dcp_df)\n",
    "    dcp_df = dcp_df.drop(['DOY', 'Day', 'Month', 'millis'], axis=1)\n",
    "    dcp_summer_mean_df = dcp_df.groupby(['Year']).mean(['Temp-mean']).reset_index()\n",
    "\n",
    "    # Reduce region function for historical data\n",
    "    reduce_prism = create_reduce_region_function(\n",
    "        geometry=point, reducer=ee.Reducer.first(), scale=1000, crs='EPSG:4326')\n",
    "\n",
    "    # Query historical data\n",
    "    prism_stat_fc = ee.FeatureCollection(prism_col.map(reduce_prism)).filter(\n",
    "        ee.Filter.notNull(prism_col.first().bandNames()))\n",
    "\n",
    "    prism_dict = fc_to_dict(prism_stat_fc).getInfo()\n",
    "    prism_df = pd.DataFrame(prism_dict)\n",
    "\n",
    "    # Process historical data\n",
    "    prism_df = add_date_info(prism_df)\n",
    "    prism_df['Model'] = 'PRISM'\n",
    "    prism_df = prism_df.rename(columns={'tmean': 'Temp-mean'})\n",
    "    prism_df['Temp-mean'] = (prism_df['Temp-mean'] * (9/5)) + 32\n",
    "    prism_df = filter_summer_months(prism_df)\n",
    "    prism_df = prism_df.drop(['DOY', 'Day', 'Month', 'millis'], axis=1)\n",
    "    prism_summer_mean_df = prism_df.groupby(['Year']).mean(['Temp-mean']).reset_index()\n",
    "\n",
    "    # Combine historical and projected data\n",
    "    combined_df = pd.concat([prism_summer_mean_df, dcp_summer_mean_df], ignore_index=True)\n",
    "\n",
    "    # Add columns for facility metadata\n",
    "    combined_df['facility_id'] = facility_id\n",
    "    combined_df['name'] = name\n",
    "    combined_df['latitude'] = latitude\n",
    "    combined_df['longitude'] = longitude\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Read in centroid dataset\n",
    "df = pd.read_csv('centroids.csv')\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Extract lat lon\n",
    "gdf['latitude'] = gdf.geometry.y\n",
    "gdf['longitude'] = gdf.geometry.x\n",
    "\n",
    "# List for results\n",
    "all_results = []\n",
    "\n",
    "# Retrieve total number of facilities for print statement\n",
    "total_facilities = len(gdf)\n",
    "\n",
    "# Process facilities (with timeout, retries, and logging)\n",
    "def process_with_timeout(idx, row, retries=3, timeout=30):\n",
    "    for attempt in range(retries):\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future = executor.submit(process_facility, row)\n",
    "            try:\n",
    "                combined_df = future.result(timeout=timeout)\n",
    "                all_results.append(combined_df)\n",
    "                print(f'Successfully retrieved data for Facility ID {row[\"FACILITYID\"]} (Facility {idx + 1} out of {total_facilities}).')\n",
    "                break\n",
    "            except TimeoutError:\n",
    "                print(f'Timeout retrieving data for Facility ID {row[\"FACILITYID\"]} (Facility {idx + 1} out of {total_facilities}), retrying... ({attempt + 1}/{retries})')\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)  \n",
    "            except Exception as e:\n",
    "                print(f'Error retrieving data for Facility ID {row[\"FACILITYID\"]} (Facility {idx + 1} out of {total_facilities}): {e}')\n",
    "                break\n",
    "\n",
    "# Run process function\n",
    "for idx, row in gdf.iterrows():\n",
    "    process_with_timeout(idx, row)\n",
    "\n",
    "# Concatenate results and add source label\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "final_df['source'] = final_df['Year'].apply(lambda x: 'prism' if x <= 2023 else 'nexdcp30')\n",
    "final_df.to_csv('prism-nexdcp30.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
